### **5. Spring Boot with Kafka**

#### **1. Producing and Consuming Messages**

- **Producing Messages**:
  Spring Boot provides `KafkaTemplate` to send messages to Kafka topics.
  Example:
  ```java
  @Service
  public class KafkaProducerService {
      @Autowired
      private KafkaTemplate<String, String> kafkaTemplate;

      public void sendMessage(String topic, String message) {
          kafkaTemplate.send(topic, message);
      }
  }
  ```

  You can call `sendMessage()` to produce a message:
  ```java
  producerService.sendMessage("myTopic", "Hello, Kafka!");
  ```

- **Consuming Messages**:
  Spring Boot uses `@KafkaListener` to consume messages from Kafka topics.
  Example:
  ```java
  @Service
  public class KafkaConsumerService {
      @KafkaListener(topics = "myTopic", groupId = "myGroup")
      public void listen(String message) {
          System.out.println("Received Message: " + message);
      }
  }
  ```

---

#### **2. Configuring Kafka Producer and Consumer Properties**

Spring Boot makes configuration easy using the `application.properties` or `application.yml` file.

- **Producer Properties**:
  Configure Kafka producer settings:
```
- properties
  spring.kafka.producer.bootstrap-servers=localhost:9092
  spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
  spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
  ```

- **Consumer Properties**:
  Configure Kafka consumer settings:
```
properties
  spring.kafka.consumer.bootstrap-servers=localhost:9092
  spring.kafka.consumer.group-id=myGroup
  spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
  spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
  spring.kafka.consumer.auto-offset-reset=earliest
  ```

- **Topic Configuration**:
  You can also define Kafka topics programmatically using `NewTopic`.
  ```java
  @Configuration
  public class KafkaTopicConfig {
      @Bean
      public NewTopic topic() {
          return TopicBuilder.name("myTopic").partitions(3).replicas(1).build();
      }
  }
  ```

---

#### **3. Handling Errors and Implementing Retry Logic**

- **Error Handling**:
  You can use `@KafkaListener`'s `ErrorHandler` or a custom handler to deal with errors during message consumption.
  ```java
  @Configuration
  public class KafkaErrorHandler implements ConsumerAwareErrorHandler {
      @Override
      public void handle(Exception thrownException, ConsumerRecord<?, ?> data, Consumer<?, ?> consumer) {
          System.err.println("Error occurred: " + thrownException.getMessage());
      }
  }
  ```

  Register it in your consumer factory:
  ```java
  @Configuration
  public class KafkaConsumerConfig {
      @Bean
      public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {
          ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
          factory.setConsumerFactory(consumerFactory());
          factory.setErrorHandler(new KafkaErrorHandler());
          return factory;
      }
  }
  ```

- **Retry Logic**:
  Spring Kafka allows you to implement retry logic for message consumption using `RetryTemplate` or a `SeekToCurrentErrorHandler`.
  ```java
  @Configuration
  public class KafkaRetryConfig {
      @Bean
      public ConcurrentKafkaListenerContainerFactory<String, String> retryKafkaListenerContainerFactory() {
          ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
          factory.setConsumerFactory(consumerFactory());

          factory.setErrorHandler(new SeekToCurrentErrorHandler(3)); // Retry up to 3 times
          return factory;
      }
  }
  ```

---

These are the essentials of working with Spring Boot and Kafka.

---

### **1. Advanced Kafka Producer Features**

- **Sending Messages with Keys**:
  Messages in Kafka can be sent with a key, which helps with partitioning and ordering. Messages with the same key are sent to the same partition.
  ```java
  kafkaTemplate.send("myTopic", "key1", "Message with Key");
  ```

- **Asynchronous Message Sending**:
  The `send()` method returns a `ListenableFuture` that allows handling success or failure callbacks.
  ```java
  kafkaTemplate.send("myTopic", "Message")
      .addCallback(success -> {
          System.out.println("Message sent successfully: " + success.getProducerRecord().value());
      }, failure -> {
          System.err.println("Message failed to send: " + failure.getMessage());
      });
  ```

- **Custom Partitioning**:
  You can define a custom partitioner to control how messages are distributed across partitions.
  ```java
  @Component
  public class CustomPartitioner extends DefaultPartitioner {
      @Override
      public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
          // Custom partitioning logic
          return key.hashCode() % cluster.partitionCountForTopic(topic);
      }
  }
  ```

---

### **2. Advanced Kafka Consumer Features**

- **Batch Message Consumption**:
  Spring Kafka allows batch consumption where multiple records can be fetched and processed together.
  ```java
  @KafkaListener(topics = "myTopic", groupId = "myGroup", containerFactory = "batchFactory")
  public void listenBatch(List<ConsumerRecord<String, String>> messages) {
      messages.forEach(message -> {
          System.out.println("Received: " + message.value());
      });
  }
  ```

  Configure the container factory for batch mode:
  ```java
  @Configuration
  public class KafkaConsumerConfig {
      @Bean
      public ConcurrentKafkaListenerContainerFactory<String, String> batchFactory() {
          ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
          factory.setConsumerFactory(consumerFactory());
          factory.setBatchListener(true);
          return factory;
      }
  }
  ```

- **Offset Management**:
  Kafka offsets can be managed automatically (default) or manually. For manual management:
  ```java
  @KafkaListener(topics = "myTopic", groupId = "myGroup")
  public void listenWithManualCommit(ConsumerRecord<String, String> record, Acknowledgment ack) {
      System.out.println("Processing: " + record.value());
      ack.acknowledge(); // Commit offset manually
  }
  ```

- **Consumer Rebalancing**:
  To handle consumer rebalancing events (e.g., when partitions are reassigned), you can use the `ConsumerRebalanceListener`:
  ```java
  public class MyRebalanceListener implements ConsumerRebalanceListener {
      @Override
      public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
          System.out.println("Partitions Revoked: " + partitions);
      }

      @Override
      public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
          System.out.println("Partitions Assigned: " + partitions);
      }
  }
  ```

---

### **3. Error Handling and Retry Mechanisms**

- **Error Recovery with Dead Letter Topics**:
  Messages that fail after retries can be sent to a Dead Letter Topic (DLT) for further analysis.
  ```java
  @Bean
  public DeadLetterPublishingRecoverer recoverer(KafkaTemplate<String, String> kafkaTemplate) {
      return new DeadLetterPublishingRecoverer(kafkaTemplate, (r, e) -> new TopicPartition("dead-letter-topic", 0));
  }
  ```

  Configure the error handler:
  ```java
  @Bean
  public SeekToCurrentErrorHandler errorHandler(DeadLetterPublishingRecoverer recoverer) {
      return new SeekToCurrentErrorHandler(recoverer, 3); // Retry 3 times
  }
  ```

- **Retry Template for Producers**:
  Use `RetryTemplate` for retrying message sending with backoff strategies.
  ```java
  @Bean
  public KafkaTemplate<String, String> kafkaTemplate(ProducerFactory<String, String> producerFactory) {
      KafkaTemplate<String, String> template = new KafkaTemplate<>(producerFactory);
      template.setRetryTemplate(new RetryTemplate());
      return template;
  }
  ```

---

### **4. Monitoring and Logging**

- **Metrics with Micrometer**:
  Spring Boot integrates with Micrometer to collect Kafka metrics.
  Add the dependency:
  ```xml
  <dependency>
      <groupId>io.micrometer</groupId>
      <artifactId>micrometer-core</artifactId>
  </dependency>
  ```
  Configure metrics in `application.properties`:
```
- properties
  management.metrics.enable.kafka=true
  ```

- **Logging Interceptors**:
  Use Producer and Consumer interceptors for logging message flow.
  ```java
  public class LoggingProducerInterceptor implements ProducerInterceptor<String, String> {
      @Override
      public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {
          System.out.println("Sending message: " + record.value());
          return record;
      }

      @Override
      public void onAcknowledgement(RecordMetadata metadata, Exception exception) {
          if (exception == null) {
              System.out.println("Message acknowledged: " + metadata.offset());
          } else {
              System.err.println("Message failed: " + exception.getMessage());
          }
      }
  }
  ```

---

### **5. Real-World Use Cases**

- **Event-Driven Architectures**:
  Use Kafka to build microservices that communicate asynchronously through events.

- **Stream Processing**:
  Use Kafka Streams or tools like Apache Flink for processing real-time data streams.

- **Data Pipelines**:
  Kafka can be used to ingest and distribute data in big data ecosystems with tools like Apache Spark or Hadoop.

---

Spring Boot with Kafka is a powerful combination, and you can tailor its features to handle enterprise-scale, distributed systems.

---

### **Kafka Streams: Overview**
Kafka Streams is a client library provided by Apache Kafka for building real-time, scalable, and fault-tolerant stream processing applications. It allows you to process and transform data stored in Kafka topics into meaningful results.

Key features of Kafka Streams include:
- **Declarative Programming Model**: Use the Streams DSL (Domain Specific Language) for intuitive programming.
- **Fault Tolerance**: State stores are backed by Kafka, providing recovery after failure.
- **Windowing Support**: Process data over time windows (e.g., tumbling, sliding, session).
- **Stateful Processing**: Supports maintaining local state for complex aggregations.

---

### **1. Setting Up Kafka Streams in Spring Boot**

- **Dependencies**:
  Add the Kafka Streams library to your project:
  ```xml
  <dependency>
      <groupId>org.springframework.kafka</groupId>
      <artifactId>spring-kafka</artifactId>
  </dependency>
  ```

- **Configuration**:
  Configure Kafka Streams properties in `application.properties`:
```
- properties
  spring.kafka.streams.application-id=my-streams-app
  spring.kafka.streams.bootstrap-servers=localhost:9092
  spring.kafka.streams.default-key-serde=org.apache.kafka.common.serialization.Serdes$StringSerde
  spring.kafka.streams.default-value-serde=org.apache.kafka.common.serialization.Serdes$StringSerde
  ```

---

### **2. Streams DSL: Building Real-Time Applications**

The Streams DSL allows you to process Kafka topics with a functional approach. Let's explore common operations:

#### **Mapping and Filtering**
- Use `map()` and `filter()` to transform or filter incoming data.
  ```java
  @Component
  public class StreamProcessor {
      @Bean
      public KStream<String, String> processStream(StreamsBuilder builder) {
          KStream<String, String> stream = builder.stream("input-topic");
          stream.filter((key, value) -> value.contains("important"))
                .mapValues(value -> value.toUpperCase())
                .to("output-topic");
          return stream;
      }
  }
  ```

#### **Windowing**
- Windowing enables processing over specific time intervals (e.g., calculate metrics every 5 minutes).
  ```java
  @Bean
  public KStream<String, Long> windowedStream(StreamsBuilder builder) {
      KStream<String, String> stream = builder.stream("input-topic");
      KGroupedStream<String, String> groupedStream = stream.groupByKey();

      KTable<Windowed<String>, Long> counts = groupedStream
          .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))
          .count();

      counts.toStream().to("output-topic", Produced.with(WindowedSerdes.stringSerde(), Serdes.Long()));
      return stream;
  }
  ```

#### **Joins**
- Joining streams allows you to combine data from multiple topics.
  ```java
  @Bean
  public KStream<String, String> joinedStream(StreamsBuilder builder) {
      KStream<String, String> stream1 = builder.stream("topic1");
      KStream<String, String> stream2 = builder.stream("topic2");

      KStream<String, String> joined = stream1.join(
          stream2,
          (value1, value2) -> value1 + " & " + value2,
          JoinWindows.of(Duration.ofMinutes(5))
      );
      joined.to("joined-topic");
      return joined;
  }
  ```

---

### **3. State Stores for Stateful Processing**

Kafka Streams provides **state stores** to store intermediate results during stream processing. These are local to the application and backed by Kafka for fault tolerance.

- Example: Counting Word Occurrences
  ```java
  @Bean
  public KStream<String, Long> wordCountStream(StreamsBuilder builder) {
      KStream<String, String> stream = builder.stream("input-topic");
      KGroupedStream<String, String> grouped = stream
          .flatMapValues(value -> Arrays.asList(value.split("\\s+")))
          .groupBy((key, word) -> word);

      KTable<String, Long> wordCounts = grouped.count(Materialized.as("word-store"));

      wordCounts.toStream().to("output-topic", Produced.with(Serdes.String(), Serdes.Long()));
      return stream;
  }
  ```

---

### **4. Schema Evolution with Avro Serialization**

When working with structured data, Avro is a popular choice for serialization. It ensures schema evolution, which is critical for distributed systems.

- **Dependencies**:
  Add the Avro dependencies:
  ```xml
  <dependency>
      <groupId>io.confluent</groupId>
      <artifactId>kafka-avro-serializer</artifactId>
  </dependency>
  ```

- **Defining Avro Schema**:
  Example schema for a User:
  ```json
  {
      "type": "record",
      "name": "User",
      "fields": [
          {"name": "id", "type": "string"},
          {"name": "name", "type": "string"},
          {"name": "email", "type": "string"}
      ]
  }
  ```

- **Configuring Kafka to Use Avro**:
```
properties
  spring.kafka.properties.schema.registry.url=http://localhost:8081
  spring.kafka.producer.value-serializer=io.confluent.kafka.serializers.KafkaAvroSerializer
  spring.kafka.consumer.value-deserializer=io.confluent.kafka.serializers.KafkaAvroDeserializer
```

---

### **5. Real-World Use Cases for Kafka Streams**

- **Fraud Detection**: Analyze real-time transactions to detect anomalies or fraudulent activity.
- **Monitoring and Alerts**: Process log data to generate alerts for unusual patterns.
- **Data Enrichment**: Join and enrich data streams from multiple sources (e.g., user profiles with transaction logs).
- **ETL Pipelines**: Transform and load data into data warehouses or other storage systems.

---

This should give you a strong foundation for working with Kafka Streams and advanced Kafka topics!